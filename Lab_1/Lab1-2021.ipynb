{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS4035 - Cyber Data Analytics\n",
    "## Lab 1 - Fraud data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit on brightspace (zip file with the name Group_xx.zip)\n",
    "(i) This jupyter file completed with code, plots, figures and report for each question. Additional plots and figures can be created for explanation before the end of each main question. Lab 1 contains 5 main questions, including the bonus. Write the code or explanation below each sub question. For the explanations, include what you would normally include in the report for this lab assignment, for example data pre-processing, hypothesis tested, approach, results, etc.\n",
    "(ii) The libraries needed to run this file. Except for numpy, scikit-learn, pandas, matplotlib\n",
    "\n",
    "Your peers should be able to use the readme section for instructions and be able to run this file. \n",
    "\n",
    "Make sure to keep your answers concise. Maximum number of words is 1000, which you can count with the code below. (You can add around 600 words since we start at around 400)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this cell does not work try running `pip install nbformat`\n",
    "\n",
    "import io\n",
    "import nbformat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import re\n",
    "import csv\n",
    "\n",
    "nb = nbformat.read(\"Lab1-2021.ipynb\", nbformat.NO_CONVERT)\n",
    "word_count = 0\n",
    "for cell in nb.cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "\n",
    "print(\"Word count:\", word_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Number :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student 1 \n",
    "### Name :\n",
    "### ID :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student 2\n",
    "### Name :\n",
    "### ID :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readme - Provide instructions - libraries used, location of the data file, etc. Keep it short. Remember your peers will not debug your code and should be able to reproduce the exact output you provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = \"train_data.csv\"\n",
    "df1 = pd.read_csv(data)\n",
    "\n",
    "#Prepare card id and ip id for plotting\n",
    "\n",
    "# 1.Card ID\n",
    "card_enc = LabelEncoder()\n",
    "card_enc.fit(df1['card_id'])\n",
    "df1['card_id'] = card_enc.transform(df1.card_id)\n",
    "\n",
    "# 2.IP ID\n",
    "ip_enc = LabelEncoder()\n",
    "ip_enc.fit(df1['ip_id'])\n",
    "df1['ip_id'] = ip_enc.transform(df1.ip_id)\n",
    "\n",
    "data = \"train_data.csv\"\n",
    "df1 = pd.read_csv(data)\n",
    "\n",
    "#Prepare card id and ip id for plotting\n",
    "\n",
    "# 1.Card ID\n",
    "card_enc = LabelEncoder()\n",
    "card_enc.fit(df1['card_id'])\n",
    "df1['card_id'] = card_enc.transform(df1.card_id)\n",
    "\n",
    "# 2.IP ID\n",
    "ip_enc = LabelEncoder()\n",
    "ip_enc.fit(df1['ip_id'])\n",
    "df1['ip_id'] = ip_enc.transform(df1.ip_id)\n",
    "\n",
    "#Encoding, Cleaning the data\n",
    "df1.loc[df1['cardverificationcodesupplied'].isna(),'cardverificationcodesupplied'] = False\n",
    "df1.loc[df1['issuercountrycode'].isna(),'issuercountrycode'] = 'ZZ'\n",
    "df1.loc[df1['shoppercountrycode'].isna(),'shoppercountrycode'] = 'ZZ'\n",
    "\n",
    "unique_issuer_cc = df1['issuercountrycode'].unique()\n",
    "unique_shopper_cc = df1['shoppercountrycode'].unique()\n",
    "both = np.append(unique_issuer_cc, unique_shopper_cc)\n",
    "df_countrycodes = pd.DataFrame(both)\n",
    "unique_codes = df_countrycodes[0].unique()\n",
    "enc = LabelEncoder()\n",
    "enc.fit(unique_codes)\n",
    "df1['issuercountrycode'] = enc.transform(df1.issuercountrycode)\n",
    "df1['shoppercountrycode'] = enc.transform(df1.shoppercountrycode)\n",
    "def conv(row):\n",
    "    currency_dict = {\"BGN\": 1.9558, \"NZD\": 1.6805, \"ILS\": 4.0448, \"RUB\": 72.2099, \"CAD\": 1.5075, \"USD\": 1.1218,\n",
    "                     \"PHP\": 58.125, \"CHF\": 1.1437, \"ZAR\": 16.0224, \"AUD\": 1.5911, \"JPY\": 124.93, \"TRY\": 6.6913,\n",
    "                     \"HKD\": 8.8007, \"MYR\": 4.6314, \"THB\": 35.802, \"HRK\": 7.413, \"NOK\": 9.6678, \"IDR\": 15953.68,\n",
    "                     \"DKK\": 7.4646, \"CZK\": 25.659, \"HUF\": 322.97, \"GBP\": 0.86248, \"MXN\": 21.2829, \"KRW\": 1308.01,\n",
    "                     \"ISK\": 136.2, \"SGD\": 1.5263, \"BRL\": 4.405, \"PLN\": 4.2868, \"INR\": 78.0615, \"RON\": 4.7596,\n",
    "                     \"CNY\": 7.5541, \"SEK\": 10.635}\n",
    "    return row['amount'] / (currency_dict[row['currencycode']]*100)\n",
    "\n",
    "df1['amount_eur'] = df1.apply(lambda x: conv(x), axis=1)\n",
    "\n",
    "\n",
    "enc1 = LabelEncoder()\n",
    "enc1.fit(df1['txvariantcode'])\n",
    "df1['txvariantcode'] = enc1.transform(df1.txvariantcode)\n",
    "\n",
    "enc2 = LabelEncoder()\n",
    "enc2.fit(df1['currencycode'])\n",
    "df1['currencycode'] = enc2.transform(df1.currencycode)\n",
    "\n",
    "enc3 = LabelEncoder()\n",
    "enc3.fit(df1['shopperinteraction'])\n",
    "df1['shopperinteraction'] = enc3.transform(df1.shopperinteraction)\n",
    "\n",
    "df1['accountcode'] = df1['accountcode'].apply(lambda x: re.sub('Account','',x))\n",
    "df1['accountcode_cc'] = 0\n",
    "df1.loc[(df1['accountcode'] == 'UK'),'accountcode_cc'] = 'GB'\n",
    "df1.loc[(df1['accountcode'] == 'Mexico'),'accountcode_cc'] = 'MX'\n",
    "df1.loc[(df1['accountcode'] == 'Sweden'),'accountcode_cc'] = 'SE'\n",
    "df1.loc[(df1['accountcode'] == 'APAC'),'accountcode_cc'] = 'APAC'\n",
    "\n",
    "enc4 = LabelEncoder()\n",
    "enc4.fit(df1['accountcode'])\n",
    "df1['accountcode'] = enc4.transform(df1.accountcode)\n",
    "\n",
    "enc5 = LabelEncoder()\n",
    "enc5.fit(df1['cardverificationcodesupplied'])\n",
    "df1['cardverificationcodesupplied'] = enc5.transform(df1.cardverificationcodesupplied)\n",
    "\n",
    "df1.loc[df1['mail_id'].str.contains('na',case=False),'mail_id'] = 'email99999'\n",
    "\n",
    "enc6 = LabelEncoder()\n",
    "enc6.fit(df1['mail_id'])\n",
    "df1['mail_id'] = enc6.transform(df1.mail_id)\n",
    "\n",
    "df1.loc[df1['cvcresponsecode'] > 2,'cvcresponsecode'] = 3\n",
    "\n",
    "#Feature Engineering\n",
    "df1['countries_equal'] = (df1['shoppercountrycode'] == df1['issuercountrycode'])\n",
    "df1.loc[df1['countries_equal'] == False,'countries_equal'] = 0\n",
    "df1.loc[df1['countries_equal'] == True,'countries_equal'] = 1\n",
    "\n",
    "\n",
    "X = df1[['issuercountrycode','txvariantcode','bin','amount','currencycode','shoppercountrycode','shopperinteraction','cardverificationcodesupplied','cvcresponsecode','accountcode','mail_id','ip_id','card_id','amount_eur','countries_equal']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. Visualization task â€“ 1 A4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Plot visulations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make lists of lists to group the normal transactions and the fraud transaction per CVC response code\n",
    "cvc_response_codes = []\n",
    "amounts_eur = []\n",
    "\n",
    "cvc_response_codes_fraud = []\n",
    "amounts_eur_fraud = []\n",
    "\n",
    "for i in range(4):\n",
    "    cvc_response_codes.append([])\n",
    "    amounts_eur.append([])\n",
    "\n",
    "    cvc_response_codes_fraud.append([])\n",
    "    amounts_eur_fraud.append([])\n",
    "\n",
    "#Add the values to the lists\n",
    "for row in df1.values:\n",
    "    cvc_code = row[9]\n",
    "    amount_in_euros = row[15]\n",
    "    # row[14] == 1 means a fraud transaction\n",
    "    if row[14] == 1:\n",
    "        #-0.2 so the barplots are next to each other\n",
    "        cvc_response_codes_fraud[cvc_code].append(cvc_code - 0.2)\n",
    "        amounts_eur_fraud[cvc_code].append(amount_in_euros)\n",
    "    else:\n",
    "        #+0.2 so the barplots are next to each other\n",
    "        cvc_response_codes[cvc_code].append(cvc_code + 0.2)\n",
    "        amounts_eur[cvc_code].append(amount_in_euros)\n",
    "\n",
    "number_of_normal_transactions = len([item for sublist in amounts_eur for item in sublist])\n",
    "number_of_fraud_transactions = len([item for sublist in amounts_eur_fraud for item in sublist])\n",
    "\n",
    "for i in range(len(cvc_response_codes)):\n",
    "    cvc_response_codes[i] = np.mean(cvc_response_codes[i])\n",
    "    amounts_eur[i] = len(amounts_eur[i]) / number_of_normal_transactions\n",
    "    print('Normal - cvc response code: ', i, '. Percentage:', amounts_eur[i] * 100)\n",
    "\n",
    "    if len(amounts_eur_fraud[i]) > 0:\n",
    "        cvc_response_codes_fraud[i] = np.mean(cvc_response_codes_fraud[i])\n",
    "        amounts_eur_fraud[i] = len(amounts_eur_fraud[i]) / number_of_fraud_transactions\n",
    "    else:\n",
    "        # If there are no fraud cases for a cvc code, the average is set to 0\n",
    "        cvc_response_codes_fraud[i] = np.mean(cvc_response_codes[i])\n",
    "        amounts_eur_fraud[i] = 0\n",
    "\n",
    "    print('Fraud - cvc response code: ', i, '. Percentage:', amounts_eur_fraud[i] * 100)\n",
    "\n",
    "plt.bar(cvc_response_codes, amounts_eur, color = 'b', width = 0.4, label = 'Normal')\n",
    "plt.bar(cvc_response_codes_fraud, amounts_eur_fraud, color = 'g', width = 0.4, label = 'Fraud')\n",
    "\n",
    "plt.xticks([0, 1, 2, 3], [0, 1, 2, 3])\n",
    "# Add axis labels and a title\n",
    "plt.xlabel('CVC Response Code')\n",
    "plt.ylabel('Fraction of transactions')\n",
    "plt.title('CVC Response Code vs Average Amount in Euros')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "benign_samples = df1[df1['label'] == 0]\n",
    "fraudulent_samples = df1[df1['label'] == 1]\n",
    "\n",
    "# Create a figure with a single subplot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Combine the benign and fraudulent samples data into a single list\n",
    "data = [benign_samples['amount_eur'], fraudulent_samples['amount_eur']]\n",
    "\n",
    "# Plot side-by-side boxplots\n",
    "ax.boxplot(data, labels=['Benign', 'Fraudulent'])\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Label')\n",
    "ax.set_ylabel('Amount (EUR)')\n",
    "ax.set_title('Distribution of Transaction Amounts')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# mean for amount_eur for fraud and non-fraud transactions\n",
    "print(df1.groupby('label')['amount_eur'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count the number of unique countries for benign and malicious samples\n",
    "benign = df1[df1['label'] == 0]['issuercountrycode'].nunique()\n",
    "malicious = df1[df1['label'] == 1]['issuercountrycode'].nunique()\n",
    "\n",
    "# Create a bar plot\n",
    "sns.barplot(x=['Benign', 'Malicious'], y=[benign, malicious])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Sample Type')\n",
    "plt.ylabel('Number of Unique Countries')\n",
    "plt.title('Number of Unique Countries for Benign and Malicious Samples')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Describe the visualizations and provide relavant explanations of features and relations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The first graph shows that most (90%) of the malicious transactions have a CVC response code of 0, which is unknown. Around 10% has a CVC response code of 1, which is a match. None of the malicious transactions have no match or not checked. For the normal transactions, most of the CVC response codes are a match with 80%. A bit less than 20% is unkown and there are some 'no match' and 'not checked' responses but those are so few that they are not visible in the graph."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the second graph we see the distribution of transaction amounts between the fraudulent and benign cases visualised in a boxplot. We can see that there are clearly a lot more data points for the benign cases and far fewer for the fraudulent cases. It is also interesting to see that the benign cases have much higher outliers as this indicates that there are occasional instances of very large transaction amounts in the benign cases. On the other hand, the fraudulent cases generally exhibit smaller transaction amounts with very few or no outliers. This suggests that fraudulent transactions tend to be smaller in magnitude compared to benign transactions. However, the mean of the fraudulent cases (146.04) is higher than that of the benign cases (77.47). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final graph we have visualised the number of unique countries where benign and malicious samples have been collected from. We have plotted this as a bar chart. From the visual, we can see that only a handful of countries (approximately 10) have had cases of fraudulent transaction, which indicates that fraudulent transactions are localised to a few specific countries. On the other hand, the number of countries associated with benign transactions is much larger, indicating a wider distribution of normal transactions across various regions. Additionally, the significant disparity in the number of countries between benign and malicious samples highlights the imbalanced nature of the dataset, with the majority of samples being benign transactions. It's important to consider this class imbalance when developing and evaluating machine learning models to ensure fair and accurate performance assessment.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imbalance task â€“ 1 A4 â€“ Individual"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2x. Implement Smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Print ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Analyse the performance of the classifiers. Explain which method performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Is using SMOTE a good idea? Why (not)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Privacy task â€“ 1 A4 â€“ Individual"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Print ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Analyse the performance of the classifiers. Explain which method performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Can you explain the performance difference for the different classifiers? Is it advisable to protect peopleâ€™s privacy using rank-swapping? Why (not)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification task â€“ 2 A4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Print relevant plots and metrics with clear headings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Explain the applied data pre-processing steps, learning algorithms, and post-processing steps or ensemble methods. Compare the performance of the two algorithms, focusing on performance criteria that are relevant in practice, use 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bonus task â€“ 1 A4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Provide code and report below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "bf9aa738b81349e875513c622ef6663fb616407fdaeae46f7692b5fc3bd3bcc6"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
